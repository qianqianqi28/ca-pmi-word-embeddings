{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe3de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import sqrt\n",
    "from random import Random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "#from scipy.stats.stats import spearmanr\n",
    "from scipy.stats import spearmanr\n",
    "from random import shuffle\n",
    "import math\n",
    "import random\n",
    "from numpy import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "#from sparsesvd import sparsesvd\n",
    "from numpy.linalg import matrix_rank\n",
    "from scipy.stats import chi2_contingency\n",
    "from matplotlib.pyplot import *\n",
    "from numpy import inf\n",
    "from scipy.linalg import svd\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "from foobar import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "951535a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load basic matrix and elements that are neeeded for later\n",
    "cooccur_dense = np.load(\"cooccur_dense_thr10000.npy\")\n",
    "vocab = np.load(\"vocabthr10000_dic_index.npy\", allow_pickle=True)\n",
    "vocab = dict(enumerate(vocab.flatten(), 1))[1]\n",
    "\n",
    "test_files = ['wordsim353.txt', 'men_dataset.txt', 'mturk.txt', 'rarewords.txt', 'simlex999.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "329cf943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n"
     ]
    }
   ],
   "source": [
    "dim_range1 = [2, 50]\n",
    "dim_range2 = list(np.arange(100, 1001, 100))\n",
    "dim_range3 = list(np.arange(2000, 10001, 1000))\n",
    "dim_range = [*dim_range1, *dim_range2, *dim_range3]\n",
    "print(dim_range)\n",
    "eig_weight = [0, 0.5]\n",
    "eig_weight_str = ['0', '0.5']\n",
    "rank = 15135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4adeec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(353, 294, 0.8328611898016998),\n",
       " (3000, 2159, 0.7196666666666667),\n",
       " (287, 256, 0.89198606271777),\n",
       " (2034, 294, 0.14454277286135694),\n",
       " (999, 810, 0.8108108108108109)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To look at word pairs in Wiki052024\n",
    "\n",
    "cooccur_vectors_words = fittingmatrixvectors(cooccur_dense)\n",
    "\n",
    "test_word_pairs = []\n",
    "\n",
    "for i in range(len(test_files)):\n",
    "    \n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    \n",
    "    test_word_pairs_set = read_test_word_pairs_set(path_input)\n",
    "    \n",
    "    evaluate_word_pairs_create = evaluate_word_pairs(cooccur_vectors_words, test_word_pairs_set, vocab, \"cosine\")\n",
    "    \n",
    "    test_word_pairs.append((len(test_word_pairs_set), evaluate_word_pairs_create[1], float(evaluate_word_pairs_create[1]) / len(test_word_pairs_set)))\n",
    "#table 1\n",
    "test_word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1034374",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.71 GiB for an array with shape (15135, 15135) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m     ws_score_ttest\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mttest\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_files[i], ws_evaluate(ttest_vectors_words, vocab, mearsure \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m, path \u001b[38;5;241m=\u001b[39m path_input)))      \n\u001b[0;32m     10\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable2\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mthr10000_ws_score_ttest.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, ws_score_ttest)  \n\u001b[1;32m---> 13\u001b[0m pmi \u001b[38;5;241m=\u001b[39m build_count_transform(cooccur_dense, contratype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpmi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecomposed matrix\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mthr10000_pmi.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, pmi)\n\u001b[0;32m     16\u001b[0m pmi_vectors_words \u001b[38;5;241m=\u001b[39m fittingmatrixvectors(pmi)\n",
      "File \u001b[1;32mD:\\20240901paper3\\20240920\\foobar.py:58\u001b[0m, in \u001b[0;36mbuild_count_transform\u001b[1;34m(cooccur, contratype)\u001b[0m\n\u001b[0;32m     56\u001b[0m sum_w_recip_diag \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(np\u001b[38;5;241m.\u001b[39mreciprocal(sum_w))\n\u001b[0;32m     57\u001b[0m sum_c_recip_diag \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(np\u001b[38;5;241m.\u001b[39mreciprocal(sum_c))\n\u001b[1;32m---> 58\u001b[0m pmi \u001b[38;5;241m=\u001b[39m sum_w_recip_diag \u001b[38;5;241m@\u001b[39m P \u001b[38;5;241m@\u001b[39m sum_c_recip_diag\n\u001b[0;32m     59\u001b[0m pmi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog2(pmi)\n\u001b[0;32m     60\u001b[0m pmi[pmi \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39minf] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.71 GiB for an array with shape (15135, 15135) and data type float64"
     ]
    }
   ],
   "source": [
    "#Evaluate TTEST, PMI, and PPMI\n",
    "\n",
    "ttest = build_ttest(cooccur_dense)\n",
    "np.save(\"decomposed matrix\\\\thr10000_ttest.npy\", ttest)\n",
    "ttest_vectors_words = fittingmatrixvectors(ttest)\n",
    "ws_score_ttest = []\n",
    "for i in range(len(test_files)):\n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    ws_score_ttest.append((\"ttest\", test_files[i], ws_evaluate(ttest_vectors_words, vocab, mearsure = 'cosine', path = path_input)))      \n",
    "np.save(\"table2\\\\thr10000_ws_score_ttest.npy\", ws_score_ttest)  \n",
    "del ttest\n",
    "del ttest_vectors_words\n",
    "del ws_score_ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ab022c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\20240901paper3\\20240920\\foobar.py:59: RuntimeWarning: divide by zero encountered in log2\n",
      "  pmi = np.log2(pmi)\n"
     ]
    }
   ],
   "source": [
    "pmi = build_count_transform(cooccur_dense, contratype = \"pmi\")\n",
    "np.save(\"decomposed matrix\\\\thr10000_pmi.npy\", pmi)\n",
    "\n",
    "pmi_vectors_words = fittingmatrixvectors(pmi)\n",
    "ws_score_pmi = []\n",
    "for i in range(len(test_files)):\n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    ws_score_pmi.append((\"pmi\", test_files[i], ws_evaluate(pmi_vectors_words, vocab, mearsure = 'cosine', path = path_input)))  \n",
    "np.save(\"table2\\\\thr10000_ws_score_pmi.npy\", ws_score_pmi)  \n",
    "\n",
    "del pmi\n",
    "del pmi_vectors_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e06e069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\20240901paper3\\20240920\\foobar.py:69: RuntimeWarning: divide by zero encountered in log2\n",
      "  pmi = np.log2(pmi)\n"
     ]
    }
   ],
   "source": [
    "del ws_score_pmi\n",
    "\n",
    "ppmi = build_count_transform(cooccur_dense, contratype = \"ppmi\")\n",
    "np.save(\"decomposed matrix\\\\thr10000_ppmi.npy\", ppmi)\n",
    "\n",
    "ppmi_vectors_words = fittingmatrixvectors(ppmi)\n",
    "ws_score_ppmi = []\n",
    "\n",
    "for i in range(len(test_files)):\n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    ws_score_ppmi.append((\"ppmi\", test_files[i], ws_evaluate(ppmi_vectors_words, vocab, mearsure = 'cosine', path = path_input)))    \n",
    "    \n",
    "np.save(\"table2\\\\thr10000_ws_score_ppmi.npy\", ws_score_ppmi)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "965317ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ppmi\n",
    "del ppmi_vectors_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d292b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ws_score_ppmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37a15cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ttest', 'wordsim353.txt', 0.5154054998682059),\n",
       " ('ttest', 'men_dataset.txt', 0.20805313141861317),\n",
       " ('ttest', 'mturk.txt', 0.5672027665698709),\n",
       " ('ttest', 'rarewords.txt', 0.43564018467701815),\n",
       " ('ttest', 'simlex999.txt', 0.25087934747538454)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws_score_ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4104ee9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pmi', 'wordsim353.txt', 0.22376287259551497),\n",
       " ('pmi', 'men_dataset.txt', 0.1373063844621597),\n",
       " ('pmi', 'mturk.txt', 0.4135604750664361),\n",
       " ('pmi', 'rarewords.txt', 0.25623975048136605),\n",
       " ('pmi', 'simlex999.txt', 0.18399052693539292)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws_score_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4823386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ppmi', 'wordsim353.txt', 0.6061350468104976),\n",
       " ('ppmi', 'men_dataset.txt', 0.2623660732940164),\n",
       " ('ppmi', 'mturk.txt', 0.636315760559693),\n",
       " ('ppmi', 'rarewords.txt', 0.47935612595199617),\n",
       " ('ppmi', 'simlex999.txt', 0.3522010605252525)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws_score_ppmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8d24f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposedmatrix = [\"root\", \"rootroot\"]\n",
    "#create the word-context matrix of PMI, PPMI, ROOT, ROOTROOT\n",
    "\n",
    "for i in range(len(decomposedmatrix)):\n",
    "    temp = build_count_transform(cooccur_dense, contratype = decomposedmatrix[i])\n",
    "    np.save(\"decomposed matrix\\\\thr10000_\"+decomposedmatrix[i]+\".npy\", temp)\n",
    "#copy cooccur_dense and change the name into text8_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3e0a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da091561",
   "metadata": {},
   "outputs": [],
   "source": [
    "thr10000_root = np.load(\"decomposed matrix\\\\thr10000_root.npy\")\n",
    "thr10000_root_ttest = build_ttest(thr10000_root)\n",
    "np.save(\"decomposed matrix\\\\thr10000_root_ttest.npy\", thr10000_root_ttest)\n",
    "del thr10000_root\n",
    "del thr10000_root_ttest\n",
    "\n",
    "thr10000_rootroot = np.load(\"decomposed matrix\\\\thr10000_rootroot.npy\")\n",
    "thr10000_rootroot_ttest = build_ttest(thr10000_rootroot)\n",
    "np.save(\"decomposed matrix\\\\thr10000_rootroot_ttest.npy\", thr10000_rootroot_ttest)\n",
    "del thr10000_rootroot\n",
    "del thr10000_rootroot_ttest\n",
    "\n",
    "#create the word-context matrix of STRATOS-TTEST\n",
    "\n",
    "thr10000_rootcca = build_count_transform(cooccur_dense, contratype = \"rootcca\")\n",
    "np.save(\"decomposed matrix\\\\thr10000_rootcca.npy\", thr10000_rootcca)\n",
    "del thr10000_rootcca\n",
    "\n",
    "thr10000_pmi = np.load(\"decomposed matrix\\\\thr10000_pmi.npy\")\n",
    "\n",
    "\n",
    "beta = 1\n",
    "a = 1\n",
    "b = 1\n",
    "P = cooccur_dense / np.sum(cooccur_dense)\n",
    "sum_w = np.array(P.sum(axis = 1))\n",
    "sum_c = np.array(P.sum(axis=0)).T\n",
    "sum_c_beta = (sum_c ** (beta)) / np.sum(sum_c ** (beta)) \n",
    "sum_w_pro_root = sum_w ** (a/2)\n",
    "sum_c_pro_root = sum_c_beta ** (b/2)\n",
    "        \n",
    "sum_w_pro_root_diag = np.diag(sum_w_pro_root)\n",
    "sum_c_pro_root_diag = np.diag(sum_c_pro_root)\n",
    "  \n",
    "thr10000_pmi_gsvd = sum_w_pro_root_diag @ thr10000_pmi @ sum_c_pro_root_diag\n",
    "np.save(\"decomposed matrix\\\\thr10000_pmi_gsvd.npy\", thr10000_pmi_gsvd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c4ce112",
   "metadata": {},
   "outputs": [],
   "source": [
    "del beta\n",
    "del a\n",
    "del b\n",
    "del P\n",
    "del sum_w\n",
    "del sum_c\n",
    "del sum_c_beta\n",
    "del sum_w_pro_root\n",
    "del sum_c_pro_root\n",
    "del sum_w_pro_root_diag\n",
    "del sum_c_pro_root_diag\n",
    "del thr10000_pmi_gsvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7b1438e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('root_ttest', 'wordsim353.txt', 0.5881215979327212), ('root_ttest', 'men_dataset.txt', 0.2514605903730615), ('root_ttest', 'mturk.txt', 0.6572925055943496), ('root_ttest', 'rarewords.txt', 0.4966643225577744), ('root_ttest', 'simlex999.txt', 0.34252519890148464)]\n",
      "[('root_ttest', 'wordsim353.txt', 0.5881215979327212), ('root_ttest', 'men_dataset.txt', 0.2514605903730615), ('root_ttest', 'mturk.txt', 0.6572925055943496), ('root_ttest', 'rarewords.txt', 0.4966643225577744), ('root_ttest', 'simlex999.txt', 0.34252519890148464)]\n",
      "[('rootroot_ttest', 'wordsim353.txt', 0.5067348851208608), ('rootroot_ttest', 'men_dataset.txt', 0.2358313083585509), ('rootroot_ttest', 'mturk.txt', 0.619517483279037), ('rootroot_ttest', 'rarewords.txt', 0.4261663785322584), ('rootroot_ttest', 'simlex999.txt', 0.3035801288086941)]\n",
      "[('rootcca', 'wordsim353.txt', 0.4381680358160836), ('rootcca', 'men_dataset.txt', 0.14091223186275023), ('rootcca', 'mturk.txt', 0.5508197251928694), ('rootcca', 'rarewords.txt', 0.3308769652739833), ('rootcca', 'simlex999.txt', 0.26307445772554383)]\n",
      "[('pmi_g', 'wordsim353.txt', 0.36937404388871087), ('pmi_g', 'men_dataset.txt', 0.17836965612078737), ('pmi_g', 'mturk.txt', 0.47731832074840475), ('pmi_g', 'rarewords.txt', 0.3118176658724936), ('pmi_g', 'simlex999.txt', 0.19241378680929302)]\n"
     ]
    }
   ],
   "source": [
    "#Evaluate ROOT-TTEST and save the evaluation\n",
    "root_ttest = np.load(\"decomposed matrix\\\\thr10000_root_ttest.npy\")\n",
    "root_ttest_vectors_words = fittingmatrixvectors(root_ttest)\n",
    "ws_score_root_ttest = []\n",
    "for i in range(len(test_files)):\n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    ws_score_root_ttest.append((\"root_ttest\", test_files[i], ws_evaluate(root_ttest_vectors_words, vocab, mearsure = 'cosine', path = path_input)))      \n",
    "np.save(\"table2\\\\thr10000_ws_score_root_ttest.npy\", ws_score_root_ttest)  \n",
    "print(ws_score_root_ttest)\n",
    "#Evaluate ROOTROOT-TTEST and save the evaluation\n",
    "del root_ttest\n",
    "del root_ttest_vectors_words\n",
    "\n",
    "print(ws_score_root_ttest)\n",
    "\n",
    "del ws_score_root_ttest\n",
    "\n",
    "rootroot_ttest = np.load(\"decomposed matrix\\\\thr10000_rootroot_ttest.npy\")\n",
    "rootroot_ttest_vectors_words = fittingmatrixvectors(rootroot_ttest)\n",
    "ws_score_rootroot_ttest = []\n",
    "for i in range(len(test_files)):\n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    ws_score_rootroot_ttest.append((\"rootroot_ttest\", test_files[i], ws_evaluate(rootroot_ttest_vectors_words, vocab, mearsure = 'cosine', path = path_input)))      \n",
    "np.save(\"table2\\\\thr10000_ws_score_rootroot_ttest.npy\", ws_score_rootroot_ttest)  \n",
    "del rootroot_ttest\n",
    "del rootroot_ttest_vectors_words\n",
    "print(ws_score_rootroot_ttest)\n",
    "del ws_score_rootroot_ttest\n",
    "\n",
    "rootcca = np.load(\"decomposed matrix\\\\thr10000_rootcca.npy\")\n",
    "rootcca_vectors_words = fittingmatrixvectors(rootcca)\n",
    "ws_score_rootcca = []\n",
    "for i in range(len(test_files)):\n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    ws_score_rootcca.append((\"rootcca\", test_files[i], ws_evaluate(rootcca_vectors_words, vocab, mearsure = 'cosine', path = path_input)))      \n",
    "np.save(\"table2\\\\thr10000_ws_score_rootcca.npy\", ws_score_rootcca)  \n",
    "del rootcca\n",
    "del rootcca_vectors_words\n",
    "\n",
    "print(ws_score_rootcca)\n",
    "del ws_score_rootcca\n",
    "\n",
    "pmi_gsvd = np.load(\"decomposed matrix\\\\thr10000_pmi_gsvd.npy\")\n",
    "\n",
    "pmi_g_vectors_words = fittingmatrixvectors(pmi_gsvd)\n",
    "ws_score_pmi_g = []\n",
    "\n",
    "for i in range(len(test_files)):\n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    ws_score_pmi_g.append((\"pmi_g\", test_files[i], ws_evaluate(pmi_g_vectors_words, vocab, mearsure = 'cosine', path = path_input)))      \n",
    "    \n",
    "np.save(\"table2\\\\thr10000_ws_score_pmi_g.npy\", ws_score_pmi_g)  \n",
    "del pmi_gsvd\n",
    "del pmi_g_vectors_words\n",
    "print(ws_score_pmi_g)\n",
    "del ws_score_pmi_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d3edd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Create and evaluate PMI-SVD, PPMI-SVD, and RAW-CA, ROOT-CA, ROOTROOT-CA\n",
    "\n",
    "svddecomposedmatrix = [\"pmi\", \"ppmi\"]\n",
    "for i in range(len(svddecomposedmatrix)):\n",
    "    print(i)\n",
    "    temp = np.load(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[i]+\".npy\")\n",
    "    svd_u, svd_s, svd_v = svdcadense(temp, method = \"svd\")\n",
    "    np.save(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[i]+\"_svd_u.npy\", svd_u)\n",
    "    np.save(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[i]+\"_svd_s.npy\", svd_s)\n",
    "    np.save(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[i]+\"_svd_v.npy\", svd_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c62eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "del temp\n",
    "del svd_u\n",
    "del svd_s\n",
    "del svd_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ed13a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del cooccur_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eab0f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "cadecomposedmatrix = [\"raw\", \"root\", \"rootroot\"]\n",
    "for i in range(len(cadecomposedmatrix)):\n",
    "    print(i)\n",
    "    temp = np.load(\"decomposed matrix\\\\thr10000_\"+cadecomposedmatrix[i]+\"_ttest.npy\")\n",
    "    ca_u, ca_s, ca_v = svdcadense(temp, method = \"svd\")\n",
    "    np.save(\"decomposed matrix\\\\thr10000_\"+cadecomposedmatrix[i]+\"_ca_u.npy\", ca_u)\n",
    "    np.save(\"decomposed matrix\\\\thr10000_\"+cadecomposedmatrix[i]+\"_ca_s.npy\", ca_s)\n",
    "    np.save(\"decomposed matrix\\\\thr10000_\"+cadecomposedmatrix[i]+\"_ca_v.npy\", ca_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4e6ab00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#create and evaluate ROOT-CCA\n",
    "\n",
    "svddecomposedmatrix = [\"rootcca\"]\n",
    "for i in range(len(svddecomposedmatrix)):\n",
    "    print(i)\n",
    "    temp = np.load(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[i]+\".npy\")\n",
    "    svd_u, svd_s, svd_v = svdcadense(temp, method = \"svd\")\n",
    "    np.save(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[i]+\"_svd_u.npy\", svd_u)\n",
    "    np.save(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[i]+\"_svd_s.npy\", svd_s)\n",
    "    np.save(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[i]+\"_svd_v.npy\", svd_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50f69a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#Create and evaluate PMI-GSVD\n",
    "\n",
    "svddecomposedmatrix = [\"pmi_gsvd\"]\n",
    "for i in range(len(svddecomposedmatrix)):\n",
    "    print(i)\n",
    "    temp = np.load(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[i]+\".npy\")\n",
    "    gsvd_u, gsvd_s, gsvd_v = svdcadense(temp, method = \"svd\")\n",
    "    np.save(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[i]+\"_u.npy\", gsvd_u)\n",
    "    np.save(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[i]+\"_s.npy\", gsvd_s)\n",
    "    np.save(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[i]+\"_v.npy\", gsvd_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73821bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "k = wordsim353.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = men_dataset.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = mturk.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = rarewords.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = simlex999.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "1\n",
      "k = wordsim353.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = men_dataset.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = mturk.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = rarewords.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = simlex999.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "0\n",
      "k = wordsim353.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = men_dataset.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = mturk.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = rarewords.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = simlex999.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "1\n",
      "k = wordsim353.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = men_dataset.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = mturk.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = rarewords.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = simlex999.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "2\n",
      "k = wordsim353.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = men_dataset.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = mturk.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = rarewords.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = simlex999.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "0\n",
      "k = wordsim353.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = men_dataset.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = mturk.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = rarewords.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = simlex999.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "0\n",
      "k = wordsim353.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = men_dataset.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = mturk.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = rarewords.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "k = simlex999.txt\n",
      "i = 0\n",
      "i = 0.5\n"
     ]
    }
   ],
   "source": [
    "svddecomposedmatrix = [\"pmi\", \"ppmi\"]\n",
    "\n",
    "for q in range(len(svddecomposedmatrix)):\n",
    "    print(q)\n",
    "    svd_u = np.load(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[q]+\"_svd_u.npy\")\n",
    "    svd_s = np.load(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[q]+\"_svd_s.npy\")\n",
    "    svd_ws_score_cos_all = list()\n",
    "    for k in range(len(test_files)):\n",
    "        print(\"k = \" + str(test_files[k]))\n",
    "        test_type = test_files[k]\n",
    "    \n",
    "        path_input = \"similarities\\\\\"+test_files[k]\n",
    "\n",
    "        svd_ws_score_cos = pd.DataFrame()\n",
    "    \n",
    "        for i in range(len(eig_weight)):\n",
    "            print(\"i = \" + str(eig_weight[i]))\n",
    "    \n",
    "            svd_w = svd_u * np.power(svd_s, eig_weight[i])\n",
    "\n",
    "            svd_score = []\n",
    "\n",
    "            for j in dim_range:\n",
    "                vectors_words = svdvectorsincrease(svd_w, dim = j)\n",
    "        \n",
    "                svd_score_dim = ws_evaluate(vectors_words, vocab, mearsure = 'cosine', path = path_input)\n",
    "            \n",
    "                svd_score.append(svd_score_dim)\n",
    "\n",
    "            svd_ws_score_cos[eig_weight_str[i]] = svd_score\n",
    "        svd_ws_score_cos_all.append(svd_ws_score_cos)\n",
    "    \n",
    "    np.save(\"thr10000_\"+svddecomposedmatrix[q]+\"_svd_ws_score_cos_all.npy\", svd_ws_score_cos_all)\n",
    "\n",
    "    \n",
    "cadecomposedmatrix = [\"raw\", \"root\", \"rootroot\"]\n",
    "\n",
    "for q in range(len(cadecomposedmatrix)):\n",
    "    print(q)\n",
    "    ca_u = np.load(\"decomposed matrix\\\\thr10000_\"+cadecomposedmatrix[q]+\"_ca_u.npy\")\n",
    "    ca_s = np.load(\"decomposed matrix\\\\thr10000_\"+cadecomposedmatrix[q]+\"_ca_s.npy\")\n",
    "    ca_ws_score_cos_all = list()\n",
    "    for k in range(len(test_files)):\n",
    "        print(\"k = \" + str(test_files[k]))\n",
    "        test_type = test_files[k]\n",
    "    \n",
    "        path_input = \"similarities\\\\\"+test_files[k]\n",
    "\n",
    "        ca_ws_score_cos = pd.DataFrame()\n",
    "    \n",
    "        for i in range(len(eig_weight)):\n",
    "            print(\"i = \" + str(eig_weight[i]))\n",
    "    \n",
    "            ca_w = ca_u * np.power(ca_s, eig_weight[i])\n",
    "\n",
    "            ca_score = []\n",
    "\n",
    "            for j in dim_range:\n",
    "                vectors_words = svdvectorsincrease(ca_w, dim = j)\n",
    "        \n",
    "                ca_score_dim = ws_evaluate(vectors_words, vocab, mearsure = 'cosine', path = path_input)\n",
    "            \n",
    "                ca_score.append(ca_score_dim)\n",
    "\n",
    "            ca_ws_score_cos[eig_weight_str[i]] = ca_score\n",
    "        ca_ws_score_cos_all.append(ca_ws_score_cos)\n",
    "    \n",
    "    np.save(\"thr10000_\"+cadecomposedmatrix[q]+\"_ca_ws_score_cos_all.npy\", ca_ws_score_cos_all)    \n",
    "    \n",
    "    \n",
    "svddecomposedmatrix = [\"rootcca\"]\n",
    "\n",
    "for q in range(len(svddecomposedmatrix)):\n",
    "    print(q)\n",
    "    svd_u = np.load(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[q]+\"_svd_u.npy\")\n",
    "    svd_s = np.load(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[q]+\"_svd_s.npy\")\n",
    "    svd_ws_score_cos_all = list()\n",
    "    for k in range(len(test_files)):\n",
    "        print(\"k = \" + str(test_files[k]))\n",
    "        test_type = test_files[k]\n",
    "    \n",
    "        path_input = \"similarities\\\\\"+test_files[k]\n",
    "\n",
    "        svd_ws_score_cos = pd.DataFrame()\n",
    "    \n",
    "        for i in range(len(eig_weight)):\n",
    "            print(\"i = \" + str(eig_weight[i]))\n",
    "    \n",
    "            svd_w = svd_u * np.power(svd_s, eig_weight[i])\n",
    "\n",
    "            svd_score = []\n",
    "\n",
    "            for j in dim_range:\n",
    "                vectors_words = svdvectorsincrease(svd_w, dim = j)\n",
    "        \n",
    "                svd_score_dim = ws_evaluate(vectors_words, vocab, mearsure = 'cosine', path = path_input)\n",
    "            \n",
    "                svd_score.append(svd_score_dim)\n",
    "\n",
    "            svd_ws_score_cos[eig_weight_str[i]] = svd_score\n",
    "        svd_ws_score_cos_all.append(svd_ws_score_cos)\n",
    "    \n",
    "    np.save(\"thr10000_\"+svddecomposedmatrix[q]+\"_svd_ws_score_cos_all.npy\", svd_ws_score_cos_all)\n",
    "    \n",
    "svddecomposedmatrix = [\"pmi\"]\n",
    "\n",
    "for q in range(len(svddecomposedmatrix)):\n",
    "    print(q)\n",
    "    svd_u = np.load(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[q]+\"_gsvd_u.npy\")\n",
    "    svd_s = np.load(\"decomposed matrix\\\\thr10000_\"+svddecomposedmatrix[q]+\"_gsvd_s.npy\")\n",
    "    gsvd_ws_score_cos_all = list()\n",
    "    for k in range(len(test_files)):\n",
    "        print(\"k = \" + str(test_files[k]))\n",
    "        test_type = test_files[k]\n",
    "    \n",
    "        path_input = \"similarities\\\\\"+test_files[k]\n",
    "\n",
    "        svd_ws_score_cos = pd.DataFrame()\n",
    "    \n",
    "        for i in range(len(eig_weight)):\n",
    "            print(\"i = \" + str(eig_weight[i]))\n",
    "    \n",
    "            svd_w = svd_u * np.power(svd_s, eig_weight[i])\n",
    "\n",
    "            svd_score = []\n",
    "\n",
    "            for j in dim_range:\n",
    "                vectors_words = svdvectorsincrease(svd_w, dim = j)\n",
    "        \n",
    "                svd_score_dim = ws_evaluate(vectors_words, vocab, mearsure = 'cosine', path = path_input)\n",
    "            \n",
    "                svd_score.append(svd_score_dim)\n",
    "\n",
    "            svd_ws_score_cos[eig_weight_str[i]] = svd_score\n",
    "        gsvd_ws_score_cos_all.append(svd_ws_score_cos)\n",
    "    \n",
    "    np.save(\"thr10000_\"+svddecomposedmatrix[q]+\"_gsvd_ws_score_cos_all.npy\", gsvd_ws_score_cos_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
