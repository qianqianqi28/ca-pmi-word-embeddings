{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3bf50d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import sqrt\n",
    "from random import Random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "#from scipy.stats.stats import spearmanr\n",
    "from scipy.stats import spearmanr\n",
    "from random import shuffle\n",
    "import math\n",
    "import random\n",
    "from numpy import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "#from sparsesvd import sparsesvd\n",
    "from numpy.linalg import matrix_rank\n",
    "from scipy.stats import chi2_contingency\n",
    "from matplotlib.pyplot import *\n",
    "from numpy import inf\n",
    "from scipy.linalg import svd\n",
    "from foobar import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3bc204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the .txt file and process the word embeddings\n",
    "with open('fine_tune_bert_embeddings.txt', 'r', encoding='utf-8') as file:\n",
    "    bert_vocab = []\n",
    "    bert_vectors = []\n",
    "\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        #print(values)\n",
    "        word = values[0]  # First element is the word\n",
    "        #print(word)\n",
    "        vector = np.array(values[1:], dtype='float64')  # Remaining elements are the vector\n",
    "\n",
    "        bert_vocab.append(word)  # Store word in bert_vocab list\n",
    "        bert_vectors.append(vector)  # Store vector in bert_vectors list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "770bbff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15135"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee776b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15135"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "665adf73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "629cc29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15135\n",
      "15135\n",
      "fine_tune_bert_vectors saved to fine_tune_bert_vectors.npy\n",
      "The words are in the same length.\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays\n",
    "# Start from 5 due to ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
    "bert_vocab = bert_vocab\n",
    "bert_vectors = np.array(bert_vectors)\n",
    "print(len(bert_vocab))\n",
    "print(len(bert_vectors))\n",
    "# Save vectors as separate .npy files\n",
    "np.save('fine_tune_bert_vectors.npy', bert_vectors)\n",
    "\n",
    "print(\"fine_tune_bert_vectors saved to fine_tune_bert_vectors.npy\")\n",
    "\n",
    "vocab = np.load(\"D://20240901paper3//20240920//vocabthr10000_dic_index.npy\", allow_pickle=True)\n",
    "vocab = dict(enumerate(vocab.flatten(), 1))[1]\n",
    "#print(bert_vocab[0:8])\n",
    "#print(vocab.keys())\n",
    "if len(bert_vocab) == len(vocab):\n",
    "    print(\"The words are in the same length.\")\n",
    "else:\n",
    "    print(\"The words are not in the same length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da06ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load basic matrix and elements that are neeeded for later\n",
    "bert_vectors = np.load(\"fine_tune_bert_vectors.npy\")\n",
    "test_files = ['wordsim353.txt', 'men_dataset.txt', 'mturk.txt', 'rarewords.txt', 'simlex999.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5905acf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bert_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85fc23d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(353, 294, 0.8328611898016998),\n",
       " (3000, 2159, 0.7196666666666667),\n",
       " (287, 256, 0.89198606271777),\n",
       " (2034, 294, 0.14454277286135694),\n",
       " (999, 810, 0.8108108108108109)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To look at word pairs in Text8\n",
    "\n",
    "bert_vectors_words = fittingmatrixvectors(bert_vectors)\n",
    "\n",
    "test_word_pairs = []\n",
    "\n",
    "for i in range(len(test_files)):\n",
    "    \n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    \n",
    "    test_word_pairs_set = read_test_word_pairs_set(path_input)\n",
    "    \n",
    "    evaluate_word_pairs_create = evaluate_word_pairs(bert_vectors_words, test_word_pairs_set, vocab, \"cosine\")\n",
    "    \n",
    "    test_word_pairs.append((len(test_word_pairs_set), evaluate_word_pairs_create[1], float(evaluate_word_pairs_create[1]) / len(test_word_pairs_set)))\n",
    "#table 1\n",
    "test_word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0953241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15135, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_vectors_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f19396d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bert', 'wordsim353.txt', 0.37347040289017297),\n",
       " ('bert', 'men_dataset.txt', 0.2097590663882973),\n",
       " ('bert', 'mturk.txt', 0.3476584956700762),\n",
       " ('bert', 'rarewords.txt', 0.29688535873953764),\n",
       " ('bert', 'simlex999.txt', 0.23967254446553732)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws_score_bert = []\n",
    "for i in range(len(test_files)):\n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    ws_score_bert.append((\"bert\", test_files[i], ws_evaluate(bert_vectors_words, vocab, mearsure = 'cosine', path = path_input)))  \n",
    "np.save(\"thr10000_ws_score_bert.npy\", ws_score_bert)  \n",
    "ws_score_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d23db843",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_score_bert = np.load(\"thr10000_ws_score_bert.npy\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ddac9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['bert', 'wordsim353.txt', '0.37347040289017297'],\n",
       "       ['bert', 'men_dataset.txt', '0.2097590663882973'],\n",
       "       ['bert', 'mturk.txt', '0.3476584956700762'],\n",
       "       ['bert', 'rarewords.txt', '0.29688535873953764'],\n",
       "       ['bert', 'simlex999.txt', '0.23967254446553732']], dtype='<U32')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws_score_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "079efae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4674458681536215"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(ws_score_bert[0][2].astype('float')+ws_score_bert[1][2].astype('float')+ws_score_bert[2][2].astype('float')+ws_score_bert[3][2].astype('float')+ws_score_bert[4][2].astype('float'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
