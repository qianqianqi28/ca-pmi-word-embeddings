{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac658b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import sqrt\n",
    "from random import Random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "#from scipy.stats.stats import spearmanr\n",
    "from scipy.stats import spearmanr\n",
    "from random import shuffle\n",
    "import math\n",
    "import random\n",
    "from numpy import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "#from sparsesvd import sparsesvd\n",
    "from numpy.linalg import matrix_rank\n",
    "from scipy.stats import chi2_contingency\n",
    "from matplotlib.pyplot import *\n",
    "from numpy import inf\n",
    "from scipy.linalg import svd\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "from foobar import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43184b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load basic matrix and elements that are neeeded for later\n",
    "cooccur_dense = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\cooccur_dense.npy\")\n",
    "vocab = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\vocab.npy\", allow_pickle=True)\n",
    "vocab = dict(enumerate(vocab.flatten(), 1))[1]\n",
    "\n",
    "test_files = ['wordsim353.txt', 'men_dataset.txt', 'mturk.txt', 'rarewords.txt', 'simlex999.txt']\n",
    "\n",
    "dim_range1 = [2, 50]\n",
    "dim_range2 = list(np.arange(100, 1001, 100))\n",
    "dim_range3 = list(np.arange(2000, 10001, 1000))\n",
    "dim_range4 = [11000]\n",
    "dim_range = [*dim_range1, *dim_range2, *dim_range3, *dim_range4]\n",
    "dim_range\n",
    "eig_weight = [0, 0.5, 1, 1.2]\n",
    "eig_weight_str = ['0', '0.5', '1', '1.2']\n",
    "text8_rank = 11815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b5fbcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(353, 277, 0.7847025495750708),\n",
       " (3000, 1544, 0.5146666666666667),\n",
       " (287, 221, 0.7700348432055749),\n",
       " (2034, 205, 0.1007866273352999),\n",
       " (999, 726, 0.7267267267267268)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To look at word pairs in Text8\n",
    "\n",
    "cooccur_vectors_words = fittingmatrixvectors(cooccur_dense)\n",
    "\n",
    "text8_test_word_pairs = []\n",
    "\n",
    "for i in range(len(test_files)):\n",
    "    \n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    \n",
    "    test_word_pairs_set = read_test_word_pairs_set(path_input)\n",
    "    \n",
    "    evaluate_word_pairs_create = evaluate_word_pairs(cooccur_vectors_words, test_word_pairs_set, vocab, \"cosine\")\n",
    "    \n",
    "    text8_test_word_pairs.append((len(test_word_pairs_set), evaluate_word_pairs_create[1], float(evaluate_word_pairs_create[1]) / len(test_word_pairs_set)))\n",
    "#table 1\n",
    "text8_test_word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e0b7c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.100e+01, 1.000e+00, 2.200e+01, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [1.000e+00, 0.000e+00, 5.900e+01, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [2.200e+01, 5.900e+01, 9.282e+03, ..., 4.000e+00, 0.000e+00,\n",
       "        2.500e+00],\n",
       "       ...,\n",
       "       [0.000e+00, 0.000e+00, 4.000e+00, ..., 3.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 8.900e+01,\n",
       "        0.000e+00],\n",
       "       [0.000e+00, 0.000e+00, 2.500e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooccur_vectors_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d015acd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qi000005\\OneDrive - Universiteit Utrecht\\qi000005\\paper 3\\Correspondence analysis to PMI-based word embeddings\\CARME\\foobar.py:86: RuntimeWarning: divide by zero encountered in log2\n",
      "  pmi = np.log2(pmi)\n"
     ]
    }
   ],
   "source": [
    "#Evaluate TTEST, PMI, and PPMI\n",
    "\n",
    "ttest = build_ttest(cooccur_dense)\n",
    "ttest_vectors_words = fittingmatrixvectors(ttest)\n",
    "ws_score_ttest = []\n",
    "for i in range(len(test_files)):\n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    ws_score_ttest.append((\"ttest\", test_files[i], ws_evaluate(ttest_vectors_words, vocab, mearsure = 'cosine', path = path_input)))      \n",
    "    \n",
    "\n",
    "pmi = build_count_transform(cooccur_dense, contratype = \"pmi\")\n",
    "pmi_vectors_words = fittingmatrixvectors(pmi)\n",
    "ws_score_pmi = []\n",
    "for i in range(len(test_files)):\n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    ws_score_pmi.append((\"pmi\", test_files[i], ws_evaluate(pmi_vectors_words, vocab, mearsure = 'cosine', path = path_input)))  \n",
    "\n",
    "ppmi = build_count_transform(cooccur_dense, contratype = \"ppmi\")\n",
    "ppmi_vectors_words = fittingmatrixvectors(ppmi)\n",
    "ws_score_ppmi = []\n",
    "\n",
    "for i in range(len(test_files)):\n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    ws_score_ppmi.append((\"ppmi\", test_files[i], ws_evaluate(ppmi_vectors_words, vocab, mearsure = 'cosine', path = path_input)))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d8c7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the evaluation of TTEST, PMI, and PPMI\n",
    "\n",
    "np.save(\"D:\\\\carme\\\\text8\\\\table2\\\\text8_ws_score_ttest.npy\", ws_score_ttest)  \n",
    "np.save(\"D:\\\\carme\\\\text8\\\\table2\\\\text8_ws_score_pmi.npy\", ws_score_pmi)  \n",
    "np.save(\"D:\\\\carme\\\\text8\\\\table2\\\\text8_ws_score_ppmi.npy\", ws_score_ppmi)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad5268d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the word-context matrix of PMI, PPMI, ROOT, ROOTROOT\n",
    "\n",
    "decomposedmatrix = [\"pmi\", \"ppmi\", \"root\", \"rootroot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8e57857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qi000005\\OneDrive - Universiteit Utrecht\\qi000005\\paper 3\\Correspondence analysis to PMI-based word embeddings\\CARME\\foobar.py:76: RuntimeWarning: divide by zero encountered in log2\n",
      "  pmi = np.log2(pmi)\n",
      "C:\\Users\\qi000005\\OneDrive - Universiteit Utrecht\\qi000005\\paper 3\\Correspondence analysis to PMI-based word embeddings\\CARME\\foobar.py:86: RuntimeWarning: divide by zero encountered in log2\n",
      "  pmi = np.log2(pmi)\n"
     ]
    }
   ],
   "source": [
    "#create the word-context matrix of PMI, PPMI, ROOT, ROOTROOT\n",
    "\n",
    "for i in range(len(decomposedmatrix)):\n",
    "    temp = build_count_transform(cooccur_dense, contratype = decomposedmatrix[i])\n",
    "    np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+decomposedmatrix[i]+\".npy\", temp)\n",
    "#copy cooccur_dense and change the name into text8_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3731dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate ROOT-TTEST and save the evaluation\n",
    "\n",
    "text8_root = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_root.npy\")\n",
    "root_ttest = build_ttest(text8_root)\n",
    "root_ttest_vectors_words = fittingmatrixvectors(root_ttest)\n",
    "ws_score_root_ttest = []\n",
    "for i in range(len(test_files)):\n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    ws_score_root_ttest.append((\"root_ttest\", test_files[i], ws_evaluate(root_ttest_vectors_words, vocab, mearsure = 'cosine', path = path_input)))      \n",
    "np.save(\"D:\\\\carme\\\\text8\\\\table2\\\\text8_ws_score_root_ttest.npy\", ws_score_root_ttest)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c95386d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('root_ttest', 'wordsim353.txt', 0.6578611011381043),\n",
       " ('root_ttest', 'men_dataset.txt', 0.30493595213722674),\n",
       " ('root_ttest', 'mturk.txt', 0.6655273180325981),\n",
       " ('root_ttest', 'rarewords.txt', 0.3887269669252924),\n",
       " ('root_ttest', 'simlex999.txt', 0.2756689160948789)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws_score_root_ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb85a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate ROOTROOT-TTEST and save the evaluation\n",
    "\n",
    "text8_rootroot = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_rootroot.npy\")\n",
    "rootroot_ttest = build_ttest(text8_rootroot)\n",
    "rootroot_ttest_vectors_words = fittingmatrixvectors(rootroot_ttest)\n",
    "ws_score_rootroot_ttest = []\n",
    "for i in range(len(test_files)):\n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    ws_score_rootroot_ttest.append((\"rootroot_ttest\", test_files[i], ws_evaluate(rootroot_ttest_vectors_words, vocab, mearsure = 'cosine', path = path_input)))      \n",
    "np.save(\"D:\\\\carme\\\\text8\\\\table2\\\\text8_ws_score_rootroot_ttest.npy\", ws_score_rootroot_ttest)  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70d32597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rootroot_ttest', 'wordsim353.txt', 0.6459568887285164),\n",
       " ('rootroot_ttest', 'men_dataset.txt', 0.31695810181707607),\n",
       " ('rootroot_ttest', 'mturk.txt', 0.667496372417385),\n",
       " ('rootroot_ttest', 'rarewords.txt', 0.4184534089714741),\n",
       " ('rootroot_ttest', 'simlex999.txt', 0.27058329563159905)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws_score_rootroot_ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a54f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the word-context matrix of STRATOS-TTEST\n",
    "\n",
    "decomposedmatrix = [\"rootcca\"]\n",
    "temp = build_count_transform(cooccur_dense, contratype = decomposedmatrix[0])\n",
    "np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+decomposedmatrix[0]+\".npy\", temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc73dc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rootcca', 'wordsim353.txt', 0.43803080675362516),\n",
       " ('rootcca', 'men_dataset.txt', 0.15640434025408928),\n",
       " ('rootcca', 'mturk.txt', 0.5607693998005147),\n",
       " ('rootcca', 'rarewords.txt', 0.24282863093945786),\n",
       " ('rootcca', 'simlex999.txt', 0.1811511227384172)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate the STRATOS-TTEST and save the evaluation\n",
    "\n",
    "text8_rootcca = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_rootcca.npy\")\n",
    "rootcca_vectors_words = fittingmatrixvectors(text8_rootcca)\n",
    "ws_score_rootcca = []\n",
    "for i in range(len(test_files)):\n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    ws_score_rootcca.append((\"rootcca\", test_files[i], ws_evaluate(rootcca_vectors_words, vocab, mearsure = 'cosine', path = path_input)))      \n",
    "np.save(\"D:\\\\carme\\\\text8\\\\table2\\\\text8_ws_score_rootcca.npy\", ws_score_rootcca)  \n",
    "ws_score_rootcca    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ee1b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the WPMI\n",
    "\n",
    "text8_pmi = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_pmi.npy\")\n",
    "\n",
    "beta = 1\n",
    "a = 1\n",
    "b = 1\n",
    "P = cooccur_dense / np.sum(cooccur_dense)\n",
    "sum_w = np.array(P.sum(axis = 1))\n",
    "sum_c = np.array(P.sum(axis=0)).T\n",
    "sum_c_beta = (sum_c ** (beta)) / np.sum(sum_c ** (beta)) \n",
    "sum_w_pro_root = sum_w ** (a/2)\n",
    "sum_c_pro_root = sum_c_beta ** (b/2)\n",
    "        \n",
    "sum_w_pro_root_diag = np.diag(sum_w_pro_root)\n",
    "sum_c_pro_root_diag = np.diag(sum_c_pro_root)\n",
    "  \n",
    "text8_pmi_gsvd = sum_w_pro_root_diag @ text8_pmi @ sum_c_pro_root_diag\n",
    "\n",
    "pmi_g_vectors_words = fittingmatrixvectors(text8_pmi_gsvd)\n",
    "ws_score_pmi_g = []\n",
    "\n",
    "for i in range(len(test_files)):\n",
    "    path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[i]\n",
    "    ws_score_pmi_g.append((\"pmi_g\", test_files[i], ws_evaluate(pmi_g_vectors_words, vocab, mearsure = 'cosine', path = path_input)))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3614cb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pmi_g', 'wordsim353.txt', 0.23291823726181202),\n",
       " ('pmi_g', 'men_dataset.txt', 0.13158383679416213),\n",
       " ('pmi_g', 'mturk.txt', 0.34330062939259764),\n",
       " ('pmi_g', 'rarewords.txt', 0.25164877095611277),\n",
       " ('pmi_g', 'simlex999.txt', 0.1390070899975409)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws_score_pmi_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e54639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the evaluation of WPMI\n",
    "np.save(\"D:\\\\carme\\\\text8\\\\table2\\\\text8_ws_score_pmi_g.npy\", ws_score_pmi_g)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d2ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and evaluate PMI-SVD, PPMI-SVD, and RAW-CA, ROOT-CA, ROOTROOT-CA\n",
    "\n",
    "svddecomposedmatrix = [\"pmi\", \"ppmi\"]\n",
    "for i in range(len(svddecomposedmatrix)):\n",
    "    print(i)\n",
    "    temp = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[i]+\".npy\")\n",
    "    svd_u, svd_s, svd_v = svdcadense(temp, method = \"svd\")\n",
    "    np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[i]+\"_svd_u.npy\", svd_u)\n",
    "    np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[i]+\"_svd_s.npy\", svd_s)\n",
    "    np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[i]+\"_svd_v.npy\", svd_v)\n",
    "    \n",
    "cadecomposedmatrix = [\"raw\", \"root\", \"rootroot\"]\n",
    "for i in range(len(cadecomposedmatrix)):\n",
    "    print(i)\n",
    "    temp = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+cadecomposedmatrix[i]+\".npy\")\n",
    "    ca_u, ca_s, ca_v = svdcadense(temp, method = \"ca\")\n",
    "    np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+cadecomposedmatrix[i]+\"_ca_u.npy\", ca_u)\n",
    "    np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+cadecomposedmatrix[i]+\"_ca_s.npy\", ca_s)\n",
    "    np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+cadecomposedmatrix[i]+\"_ca_v.npy\", ca_v)\n",
    "    \n",
    "\n",
    "svddecomposedmatrix = [\"pmi\", \"ppmi\"]\n",
    "\n",
    "for q in range(len(svddecomposedmatrix)):\n",
    "    print(q)\n",
    "    svd_u = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[q]+\"_svd_u.npy\")\n",
    "    svd_s = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[q]+\"_svd_s.npy\")\n",
    "    svd_ws_score_cos_all = list()\n",
    "    for k in range(len(test_files)):\n",
    "        print(\"k = \" + str(test_files[k]))\n",
    "        test_type = test_files[k]\n",
    "    \n",
    "        path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[k]\n",
    "\n",
    "        svd_ws_score_cos = pd.DataFrame()\n",
    "    \n",
    "        for i in range(len(eig_weight)):\n",
    "            print(\"i = \" + str(eig_weight[i]))\n",
    "    \n",
    "            svd_w = svd_u * np.power(svd_s, eig_weight[i])\n",
    "\n",
    "            svd_score = []\n",
    "\n",
    "            for j in dim_range:\n",
    "                vectors_words = svdvectorsincrease(svd_w, dim = j)\n",
    "        \n",
    "                svd_score_dim = ws_evaluate(vectors_words, vocab, mearsure = 'cosine', path = path_input)\n",
    "            \n",
    "                svd_score.append(svd_score_dim)\n",
    "\n",
    "            svd_ws_score_cos[eig_weight_str[i]] = svd_score\n",
    "        svd_ws_score_cos_all.append(svd_ws_score_cos)\n",
    "    \n",
    "    np.save(\"D:\\\\carme\\\\text8\\\\text8_\"+svddecomposedmatrix[q]+\"_svd_ws_score_cos_all.npy\", svd_ws_score_cos_all)\n",
    "\n",
    "    \n",
    "cadecomposedmatrix = [\"raw\", \"root\", \"rootroot\"]\n",
    "\n",
    "for q in range(len(cadecomposedmatrix)):\n",
    "    print(q)\n",
    "    ca_u = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+cadecomposedmatrix[q]+\"_ca_u.npy\")\n",
    "    ca_s = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+cadecomposedmatrix[q]+\"_ca_s.npy\")\n",
    "    ca_ws_score_cos_all = list()\n",
    "    for k in range(len(test_files)):\n",
    "        print(\"k = \" + str(test_files[k]))\n",
    "        test_type = test_files[k]\n",
    "    \n",
    "        path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[k]\n",
    "\n",
    "        ca_ws_score_cos = pd.DataFrame()\n",
    "    \n",
    "        for i in range(len(eig_weight)):\n",
    "            print(\"i = \" + str(eig_weight[i]))\n",
    "    \n",
    "            ca_w = ca_u * np.power(ca_s, eig_weight[i])\n",
    "\n",
    "            ca_score = []\n",
    "\n",
    "            for j in dim_range:\n",
    "                vectors_words = svdvectorsincrease(ca_w, dim = j)\n",
    "        \n",
    "                ca_score_dim = ws_evaluate(vectors_words, vocab, mearsure = 'cosine', path = path_input)\n",
    "            \n",
    "                ca_score.append(ca_score_dim)\n",
    "\n",
    "            ca_ws_score_cos[eig_weight_str[i]] = ca_score\n",
    "        ca_ws_score_cos_all.append(ca_ws_score_cos)\n",
    "    \n",
    "    np.save(\"D:\\\\carme\\\\text8\\\\text8_\"+cadecomposedmatrix[q]+\"_ca_ws_score_cos_all.npy\", ca_ws_score_cos_all)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b10888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "k = wordsim353.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "i = 1\n",
      "i = 1.2\n",
      "k = men_dataset.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "i = 1\n",
      "i = 1.2\n",
      "k = mturk.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "i = 1\n",
      "i = 1.2\n",
      "k = rarewords.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "i = 1\n",
      "i = 1.2\n",
      "k = simlex999.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "i = 1\n",
      "i = 1.2\n"
     ]
    }
   ],
   "source": [
    "#create and evaluate ROOT-CCA\n",
    "\n",
    "svddecomposedmatrix = [\"rootcca\"]\n",
    "for i in range(len(svddecomposedmatrix)):\n",
    "    print(i)\n",
    "    temp = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[i]+\".npy\")\n",
    "    svd_u, svd_s, svd_v = svdcadense(temp, method = \"svd\")\n",
    "    np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[i]+\"_svd_u.npy\", svd_u)\n",
    "    np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[i]+\"_svd_s.npy\", svd_s)\n",
    "    np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[i]+\"_svd_v.npy\", svd_v)\n",
    "    \n",
    "\n",
    "svddecomposedmatrix = [\"rootcca\"]\n",
    "\n",
    "for q in range(len(svddecomposedmatrix)):\n",
    "    print(q)\n",
    "    svd_u = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[q]+\"_svd_u.npy\")\n",
    "    svd_s = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[q]+\"_svd_s.npy\")\n",
    "    svd_ws_score_cos_all = list()\n",
    "    for k in range(len(test_files)):\n",
    "        print(\"k = \" + str(test_files[k]))\n",
    "        test_type = test_files[k]\n",
    "    \n",
    "        path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[k]\n",
    "\n",
    "        svd_ws_score_cos = pd.DataFrame()\n",
    "    \n",
    "        for i in range(len(eig_weight)):\n",
    "            print(\"i = \" + str(eig_weight[i]))\n",
    "    \n",
    "            svd_w = svd_u * np.power(svd_s, eig_weight[i])\n",
    "\n",
    "            svd_score = []\n",
    "\n",
    "            for j in dim_range:\n",
    "                vectors_words = svdvectorsincrease(svd_w, dim = j)\n",
    "        \n",
    "                svd_score_dim = ws_evaluate(vectors_words, vocab, mearsure = 'cosine', path = path_input)\n",
    "            \n",
    "                svd_score.append(svd_score_dim)\n",
    "\n",
    "            svd_ws_score_cos[eig_weight_str[i]] = svd_score\n",
    "        svd_ws_score_cos_all.append(svd_ws_score_cos)\n",
    "    \n",
    "    np.save(\"D:\\\\carme\\\\text8\\\\text8_\"+svddecomposedmatrix[q]+\"_svd_ws_score_cos_all.npy\", svd_ws_score_cos_all)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c43ad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "11815\n",
      "1\n",
      "11815\n",
      "2\n",
      "11815\n",
      "3\n",
      "11815\n",
      "4\n",
      "11815\n",
      "5\n",
      "11815\n"
     ]
    }
   ],
   "source": [
    "#Calculate the rank of RAW, PMI, PPMI, ROOT, ROOTROOT, STRATOS-TTEST\n",
    "\n",
    "svddecomposedmatrix = [\"raw\", \"pmi\", \"ppmi\", \"root\", \"rootroot\", \"rootcca\"]\n",
    "for i in range(len(svddecomposedmatrix)):\n",
    "    print(i)\n",
    "    temp = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[i]+\".npy\")\n",
    "    print(matrix_rank(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b4d80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "k = wordsim353.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "i = 1\n",
      "i = 1.2\n",
      "k = men_dataset.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "i = 1\n",
      "i = 1.2\n",
      "k = mturk.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "i = 1\n",
      "i = 1.2\n",
      "k = rarewords.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "i = 1\n",
      "i = 1.2\n",
      "k = simlex999.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "i = 1\n",
      "i = 1.2\n",
      "1\n",
      "k = wordsim353.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "i = 1\n",
      "i = 1.2\n",
      "k = men_dataset.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "i = 1\n",
      "i = 1.2\n",
      "k = mturk.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "i = 1\n",
      "i = 1.2\n",
      "k = rarewords.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "i = 1\n",
      "i = 1.2\n",
      "k = simlex999.txt\n",
      "i = 0\n",
      "i = 0.5\n",
      "i = 1\n",
      "i = 1.2\n"
     ]
    }
   ],
   "source": [
    "#Create and evaluate PMI-GSVD\n",
    "\n",
    "svddecomposedmatrix = [\"pmi\"]\n",
    "for i in range(len(svddecomposedmatrix)):\n",
    "    print(i)\n",
    "    temp = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[i]+\".npy\")\n",
    "    gsvd_u, gsvd_s, gsvd_v = build_gsvd(temp, cooccur_dense, 1, 1)\n",
    "    np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[i]+\"_gsvd_u.npy\", gsvd_u)\n",
    "    np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[i]+\"_gsvd_s.npy\", gsvd_s)\n",
    "    np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[i]+\"_gsvd_v.npy\", gsvd_v)\n",
    "\n",
    "\n",
    "svddecomposedmatrix = [\"pmi\"]\n",
    "\n",
    "for q in range(len(svddecomposedmatrix)):\n",
    "    print(q)\n",
    "    svd_u = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[q]+\"_gsvd_u.npy\")\n",
    "    svd_s = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_\"+svddecomposedmatrix[q]+\"_gsvd_s.npy\")\n",
    "    gsvd_ws_score_cos_all = list()\n",
    "    for k in range(len(test_files)):\n",
    "        print(\"k = \" + str(test_files[k]))\n",
    "        test_type = test_files[k]\n",
    "    \n",
    "        path_input = \"D:\\\\carme\\\\test dataset\\\\similarities\\\\\"+test_files[k]\n",
    "\n",
    "        svd_ws_score_cos = pd.DataFrame()\n",
    "    \n",
    "        for i in range(len(eig_weight)):\n",
    "            print(\"i = \" + str(eig_weight[i]))\n",
    "    \n",
    "            svd_w = svd_u * np.power(svd_s, eig_weight[i])\n",
    "\n",
    "            svd_score = []\n",
    "\n",
    "            for j in dim_range:\n",
    "                vectors_words = svdvectorsincrease(svd_w, dim = j)\n",
    "        \n",
    "                svd_score_dim = ws_evaluate(vectors_words, vocab, mearsure = 'cosine', path = path_input)\n",
    "            \n",
    "                svd_score.append(svd_score_dim)\n",
    "\n",
    "            svd_ws_score_cos[eig_weight_str[i]] = svd_score\n",
    "        gsvd_ws_score_cos_all.append(svd_ws_score_cos)\n",
    "    \n",
    "    np.save(\"D:\\\\carme\\\\text8\\\\text8_\"+svddecomposedmatrix[q]+\"_gsvd_ws_score_cos_all.npy\", gsvd_ws_score_cos_all)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c7dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create TTEST, WPMI, ROOT-TTEST, ROOTROOT-TTEST\n",
    "\n",
    "text8_ttest = build_ttest(text8_cooccur_dense)\n",
    "np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_ttest.npy\", text8_ttest)\n",
    "\n",
    "text8_pmi = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_pmi.npy\")\n",
    "\n",
    "#text8_ppmi = np.where(text8_pmi < 0, 0, text8_pmi)\n",
    "\n",
    "beta = 1\n",
    "a = 1\n",
    "b = 1\n",
    "P = text8_cooccur_dense / np.sum(text8_cooccur_dense)\n",
    "sum_w = np.array(P.sum(axis = 1))\n",
    "sum_c = np.array(P.sum(axis=0)).T\n",
    "sum_c_beta = (sum_c ** (beta)) / np.sum(sum_c ** (beta)) \n",
    "sum_w_pro_root = sum_w ** (a/2)\n",
    "sum_c_pro_root = sum_c_beta ** (b/2)\n",
    "        \n",
    "sum_w_pro_root_diag = np.diag(sum_w_pro_root)\n",
    "sum_c_pro_root_diag = np.diag(sum_c_pro_root)\n",
    "  \n",
    "text8_pmi_gsvd = sum_w_pro_root_diag @ text8_pmi @ sum_c_pro_root_diag\n",
    "np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_pmi_gsvd.npy\", text8_pmi_gsvd)\n",
    "\n",
    "\n",
    "\n",
    "text8_root = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_root.npy\")\n",
    "text8_root_ttest = build_ttest(text8_root)\n",
    "np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_root_ttest.npy\", text8_root_ttest)\n",
    "\n",
    "\n",
    "text8_rootroot = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_rootroot.npy\")\n",
    "text8_rootroot_ttest = build_ttest(text8_rootroot)\n",
    "np.save(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_rootroot_ttest.npy\", text8_rootroot_ttest)\n",
    "\n",
    "#text8_rootcca = np.load(\"D:\\\\carme\\\\text8\\\\decomposed matrix\\\\text8_rootcca.npy\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
